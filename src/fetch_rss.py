import feedparser
import hashlib
import re
import json
import os
import logging
from langdetect import detect, LangDetectException
from src.config import (
    AUDIO_DIR, LOCAL_DATA_DIR, S3_OBJECT_DATA_DIR
)

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logger = logging.getLogger(__name__)

# processed_article_file
processed_articles_filepath = 'processed_article_ids.json'


def is_english_content(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆãŒè‹±èªã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹
    
    Args:
        text: åˆ¤å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        bool: è‹±èªãªã‚‰Trueã€ãã‚Œä»¥å¤–ãªã‚‰False
    """
    if not text:
        return False
        
    try:
        # HTML ã‚¿ã‚°ã‚’å‰Šé™¤
        clean_text = re.sub(r'<.*?>', '', text)
        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯æœ€åˆã®500æ–‡å­—ã ã‘åˆ¤å®š
        if len(clean_text) > 500:
            clean_text = clean_text[:500]
            
        lang = detect(clean_text)
        return lang == 'en'
    except LangDetectException:
        logger.warning(f"è¨€èªåˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {text[:50]}...")
        return False


def load_processed_ids(is_lambda=False, s3_bucket=None):
    """
    éå»ã«å‡¦ç†ã—ãŸè¨˜äº‹IDã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™

    Args:
        is_lambda: Lambdaç’°å¢ƒã‹ã©ã†ã‹
        s3_bucket: S3ãƒã‚±ãƒƒãƒˆåï¼ˆLambdaç’°å¢ƒã®å ´åˆï¼‰

    Returns:
        list: éå»ã«å‡¦ç†ã—ãŸè¨˜äº‹IDã®ãƒªã‚¹ãƒˆ
    """
    processed_ids = []

    if is_lambda and s3_bucket:
        # Lambdaç’°å¢ƒã§ã¯S3ã‹ã‚‰èª­ã¿è¾¼ã¿
        import boto3
        s3_client = boto3.client('s3')
        try:
            response = s3_client.get_object(
                Bucket=s3_bucket,
                Key=f"{S3_OBJECT_DATA_DIR}/{processed_articles_filepath}"
            )
            processed_ids = json.loads(response['Body'].read().decode('utf-8'))
            logger.info(f"S3ã‹ã‚‰{len(processed_ids)}ä»¶ã®å‡¦ç†æ¸ˆã¿è¨˜äº‹IDã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e:
            logger.info(f"å‡¦ç†æ¸ˆã¿è¨˜äº‹IDãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ã‹èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {str(e)}")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        file_path = f"{LOCAL_DATA_DIR}/{processed_articles_filepath}"
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    processed_ids = json.loads(f.read())
                logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰{len(processed_ids)}ä»¶ã®å‡¦ç†æ¸ˆã¿è¨˜äº‹IDã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            except Exception as e:
                logger.error(f"å‡¦ç†æ¸ˆã¿è¨˜äº‹IDãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        else:
            logger.info("å‡¦ç†æ¸ˆã¿è¨˜äº‹IDãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™")

    return processed_ids


def save_processed_ids(processed_ids, is_lambda=False, s3_bucket=None):
    """
    å‡¦ç†ã—ãŸè¨˜äº‹IDã®ãƒªã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã™

    Args:
        processed_ids: å‡¦ç†ã—ãŸè¨˜äº‹IDã®ãƒªã‚¹ãƒˆ
        is_lambda: Lambdaç’°å¢ƒã‹ã©ã†ã‹
        s3_bucket: S3ãƒã‚±ãƒƒãƒˆåï¼ˆLambdaç’°å¢ƒã®å ´åˆï¼‰
    """
    if is_lambda:
        file_path = f'{AUDIO_DIR}/{processed_articles_filepath}'
    else:
        os.makedirs(LOCAL_DATA_DIR, exist_ok=True)
        file_path = f"{LOCAL_DATA_DIR}/{processed_articles_filepath}"

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(processed_ids, f, ensure_ascii=False, indent=2)

    if is_lambda and s3_bucket:
        # Lambdaç’°å¢ƒã§ã¯S3ã«ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        import boto3
        s3_client = boto3.client('s3')
        try:
            s3_client.upload_file(
                file_path,
                s3_bucket,
                f"{S3_OBJECT_DATA_DIR}/{processed_articles_filepath}"
            )
            logger.info(f"å‡¦ç†æ¸ˆã¿è¨˜äº‹ID({len(processed_ids)}ä»¶)ã‚’S3ã«ä¿å­˜ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"å‡¦ç†æ¸ˆã¿è¨˜äº‹IDã®S3ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    else:
        logger.info(f"å‡¦ç†æ¸ˆã¿è¨˜äº‹ID({len(processed_ids)}ä»¶)ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ")


def fetch_rss(feed_url, is_lambda=False, s3_bucket=None):
    """
    RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã€éå»ã«å‡¦ç†ã—ãŸè¨˜äº‹ã‚’é™¤å¤–ã—ã¾ã™

    Args:
        feed_url: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®URL
        is_lambda: Lambdaç’°å¢ƒã‹ã©ã†ã‹
        s3_bucket: S3ãƒã‚±ãƒƒãƒˆåï¼ˆLambdaç’°å¢ƒã®å ´åˆï¼‰

    Returns:
        list: é‡è¤‡ã‚’é™¤å¤–ã—ãŸè¨˜äº‹ã®ãƒªã‚¹ãƒˆ
    """
    # éå»ã«å‡¦ç†ã—ãŸè¨˜äº‹IDã‚’èª­ã¿è¾¼ã¿
    processed_ids = load_processed_ids(is_lambda, s3_bucket)

    articles = []
    new_processed_ids = []

    feed = feedparser.parse(feed_url)
    for entry in feed.entries:
        original_id = entry.id

        # éå»ã«å‡¦ç†ã—ãŸè¨˜äº‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
        if original_id in processed_ids:
            logger.info(f"é‡è¤‡è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {entry.title}")
            continue  # é‡è¤‡è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—
            
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã‹ã‚‰è¨€èªåˆ¤å®š
        title_text = entry.title
        summary_text = entry.summary if hasattr(entry, 'summary') else ""
        
        # è‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        # æ—¥æœ¬èªã‚µã‚¤ãƒˆã®å ´åˆã¯ã“ã®ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        # if not is_english_content(title_text) and not is_english_content(summary_text):
        #    logger.info(f"è‹±èªä»¥å¤–ã®è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {entry.title[:30]}...")
        #    processed_ids.append(original_id)  # éè‹±èªã®è¨˜äº‹ã‚‚ã‚¹ã‚­ãƒƒãƒ—ã—ãŸè¨˜éŒ²ã«è¿½åŠ 
        #    continue

        # è¨˜äº‹ã®ä¸€æ„IDã‚’ç”Ÿæˆ
        url_hash = hashlib.md5(original_id.encode()).hexdigest()[:8]
        safe_title = re.sub(r'[^\w\s]', '', entry.title)[:30]
        safe_title = safe_title.replace(' ', '_')
        safe_id = f"{safe_title}_{url_hash}"

        articles.append({
            "id": safe_id,
            "original_id": original_id,
            "title": entry.title,
            "link": entry.link,
            "summary": entry.summary if hasattr(entry, 'summary') else "",
            "source": feed.feed.title,
            "author": entry.author if hasattr(entry, 'author') else "",
            "published": entry.published if hasattr(entry, 'published') else "",
            "source_id": feed_url.split("/")[-2] if "/" in feed_url else "unknown",
        })

        # å‡¦ç†ã—ãŸè¨˜äº‹IDã‚’è¨˜éŒ²
        new_processed_ids.append(original_id)

    # æ–°ã—ãå‡¦ç†ã—ãŸè¨˜äº‹IDã‚’æ—¢å­˜ã®ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¦ä¿å­˜
    if new_processed_ids:
        processed_ids.extend(new_processed_ids)

        # æœ€æ–°1000ä»¶ã«åˆ¶é™
        if len(processed_ids) > 1000:
            processed_ids = processed_ids[-1000:]

        save_processed_ids(processed_ids, is_lambda, s3_bucket)
        logger.info(
            f"{len(new_processed_ids)}ä»¶ã®æ–°è¦è¨˜äº‹ã‚’å‡¦ç†ã—ã¾ã—ãŸï¼ˆä¿å­˜æ¸ˆã¿è¨˜äº‹ID: {len(processed_ids)}ä»¶ï¼‰")
    else:
        logger.info("æ–°è¦è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    return articles


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    articles = fetch_rss("https://b.hatena.ne.jp/entrylist/it.rss")
    for article in articles[:3]:  # 3ä»¶ã ã‘è¡¨ç¤º
        print(f"ğŸ“Œ {article['title']} ({article['source']})")
        print(f"ğŸ”— {article['link']}")
        print(f"ğŸ“ {article['summary']}\n")
