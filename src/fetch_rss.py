import feedparser
import time
import logging
from datetime import datetime, timezone

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logger = logging.getLogger(__name__)

# processed_article_file
processed_articles_filepath = 'processed_article_ids.json'


def fetch_rss(feed_url):
    logger.info(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­: {feed_url}")
    articles = []

    try:
        feed = feedparser.parse(feed_url)
        logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰{len(feed.entries)}ä»¶ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")

        for entry in feed.entries:
            article_id = entry.get('link', entry.get('id'))
            if not article_id:
                title = entry.get('title', 'No Title')
                logger.warning(f"IDç„¡ã—è¨˜äº‹ã‚¹ã‚­ãƒƒãƒ—: {title[:50]}...")
                continue

            published_time = None
            published_parsed = entry.get('published_parsed')
            updated_parsed = entry.get('updated_parsed')

            if published_parsed:
                # time.mktime ã‚’å¤‰æ•°ã«æ ¼ç´ã—ã¦è¡Œé•·ã‚’èª¿æ•´
                ts = time.mktime(published_parsed)
                dt_naive = datetime.fromtimestamp(
                    ts
                )
                published_time = dt_naive.replace(tzinfo=timezone.utc)
            elif updated_parsed:
                # time.mktime ã‚’å¤‰æ•°ã«æ ¼ç´ã—ã¦è¡Œé•·ã‚’èª¿æ•´
                ts = time.mktime(updated_parsed)
                dt_naive = datetime.fromtimestamp(
                    ts
                )
                published_time = dt_naive.replace(tzinfo=timezone.utc)

            published_str = published_time.isoformat() if published_time else ""

            # content ã®å–å¾—ã‚’ç°¡ç•¥åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’æ”¹å–„ï¼‰
            content_list = entry.get('content', [])
            content_value = content_list[0].get(
                'value', '') if content_list else ''

            article_data = {
                'id': article_id,
                'title': entry.get('title', 'No Title'),
                'link': entry.get('link', ''),
                'published': published_str,
                'summary': entry.get('summary', ''),
                'content': content_value
            }
            articles.append(article_data)

    except Exception as e:
        logger.error(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã¾ãŸã¯è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†å®Œäº†: {feed_url}, å–å¾—è¨˜äº‹æ•°: {len(articles)}")
    return articles


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    articles = fetch_rss("https://b.hatena.ne.jp/entrylist/it.rss")
    for article in articles[:3]:  # 3ä»¶ã ã‘è¡¨ç¤º
        print(f"ğŸ“Œ {article['title']} ({article['source']})")
        print(f"ğŸ”— {article['link']}")
        print(f"ğŸ“ {article['summary']}\n")
